{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[388, 101, 105, 305], [13, 8, 175, 216], [866, 1316, 870, 1286]]\n",
      ".............................\n",
      "[[941, 673, 946, 667]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.............................\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_val)   \n\u001b[0;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m create_custom_model(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Train your model\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_dataset(data_dir):\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(data_dir, filename)) as json_file:\n",
    "                data = json.load(json_file)\n",
    "                # Assuming your images are stored in the same directory with same name but .jpg extension\n",
    "                image_path = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "                if os.path.exists(image_path):\n",
    "                    images.append(cv2.imread(image_path))\n",
    "                    # Extracting coordinates for nose tip and mouth\n",
    "                    nose_x = data[\"nose_tip\"][\"coordinates\"][\"x\"]\n",
    "                    nose_y = data[\"nose_tip\"][\"coordinates\"][\"y\"]\n",
    "                    mouth_x = data[\"mouth\"][\"coordinates\"][\"x\"]\n",
    "                    mouth_y = data[\"mouth\"][\"coordinates\"][\"y\"]\n",
    "                    annotations.append([nose_x, nose_y, mouth_x, mouth_y])\n",
    "\n",
    "    return images, annotations\n",
    "\n",
    "# Load your dataset\n",
    "data_dir = \"/home/selvatharrun/Documents/GitHub/cephalometric-analysis/train3\"\n",
    "images, annotations = load_dataset(data_dir)\n",
    "# Preprocess your data\n",
    "# You need to write code to preprocess your images and annotations\n",
    "\n",
    "# Split your dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_custom_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='linear'))  # Assuming output shape is [nose_x, nose_y, mouth_x, mouth_y]\n",
    "    return model\n",
    "\n",
    "\n",
    "print(y_train)\n",
    "print(\".............................\")\n",
    "print(y_val)   \n",
    "model = create_custom_model(X_train.shape[1:])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(np.array(X_train), np.array(y_train), \n",
    "                    validation_data=(np.array(X_val), np.array(y_val)), \n",
    "                    epochs=1, batch_size=0.2)\n",
    "\n",
    "# Evaluate your model\n",
    "# You need to write code to evaluate your model on the test set\n",
    "# Monitor training\n",
    "# Optionally, you can plot the training and validation loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image\n",
    "image = cv2.imread('example_image.jpg')\n",
    "\n",
    "# Get the shape of the image\n",
    "height, width, channels = image.shape\n",
    "\n",
    "# Print the input_shape\n",
    "input_shape = (height, width, channels)\n",
    "print(\"Input shape of the image:\", input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[245, 244, 246],\n",
      "        [244, 244, 247],\n",
      "        [245, 244, 246],\n",
      "        ...,\n",
      "        [187, 187, 187],\n",
      "        [187, 187, 187],\n",
      "        [187, 187, 187]],\n",
      "\n",
      "       [[245, 245, 246],\n",
      "        [246, 245, 248],\n",
      "        [246, 245, 247],\n",
      "        ...,\n",
      "        [187, 187, 187],\n",
      "        [187, 187, 187],\n",
      "        [187, 187, 187]],\n",
      "\n",
      "       [[246, 245, 247],\n",
      "        [245, 245, 248],\n",
      "        [245, 244, 246],\n",
      "        ...,\n",
      "        [187, 187, 187],\n",
      "        [187, 187, 187],\n",
      "        [187, 187, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 58,  34,  82],\n",
      "        [ 58,  33,  82],\n",
      "        [ 59,  35,  84],\n",
      "        ...,\n",
      "        [ 19,  27,  42],\n",
      "        [ 23,  31,  46],\n",
      "        [ 16,  22,  38]],\n",
      "\n",
      "       [[ 58,  34,  82],\n",
      "        [ 58,  33,  82],\n",
      "        [ 59,  34,  83],\n",
      "        ...,\n",
      "        [ 17,  24,  38],\n",
      "        [ 24,  32,  46],\n",
      "        [ 23,  29,  45]],\n",
      "\n",
      "       [[ 59,  35,  83],\n",
      "        [ 59,  34,  83],\n",
      "        [ 59,  34,  84],\n",
      "        ...,\n",
      "        [ 10,  16,  31],\n",
      "        [ 11,  17,  33],\n",
      "        [ 15,  22,  39]]], dtype=uint8), array([[[196, 193, 194],\n",
      "        [196, 193, 194],\n",
      "        [196, 193, 194],\n",
      "        ...,\n",
      "        [159, 159, 159],\n",
      "        [159, 159, 159],\n",
      "        [159, 159, 159]],\n",
      "\n",
      "       [[196, 193, 194],\n",
      "        [196, 193, 194],\n",
      "        [196, 193, 194],\n",
      "        ...,\n",
      "        [159, 159, 159],\n",
      "        [159, 159, 159],\n",
      "        [159, 159, 159]],\n",
      "\n",
      "       [[196, 193, 194],\n",
      "        [196, 193, 194],\n",
      "        [196, 193, 194],\n",
      "        ...,\n",
      "        [159, 159, 159],\n",
      "        [159, 159, 159],\n",
      "        [159, 159, 159]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[138, 137, 131],\n",
      "        [138, 137, 131],\n",
      "        [138, 137, 131],\n",
      "        ...,\n",
      "        [ 98, 100,  93],\n",
      "        [ 98, 100,  93],\n",
      "        [ 98, 100,  93]],\n",
      "\n",
      "       [[138, 137, 131],\n",
      "        [138, 137, 131],\n",
      "        [138, 137, 131],\n",
      "        ...,\n",
      "        [ 98, 100,  93],\n",
      "        [ 98, 100,  93],\n",
      "        [ 98, 100,  93]],\n",
      "\n",
      "       [[138, 137, 131],\n",
      "        [138, 137, 131],\n",
      "        [138, 137, 131],\n",
      "        ...,\n",
      "        [100, 102,  95],\n",
      "        [100, 102,  95],\n",
      "        [100, 102,  95]]], dtype=uint8), array([[[  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0]],\n",
      "\n",
      "       [[  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0]],\n",
      "\n",
      "       [[  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        ...,\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        ...,\n",
      "        [ 89, 103, 124],\n",
      "        [ 90, 106, 124],\n",
      "        [ 89, 101, 119]],\n",
      "\n",
      "       [[  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        ...,\n",
      "        [ 83,  96, 119],\n",
      "        [ 83,  97, 121],\n",
      "        [ 90, 104, 123]],\n",
      "\n",
      "       [[  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        [  0,   0,   0],\n",
      "        ...,\n",
      "        [ 86, 101, 120],\n",
      "        [ 86, 101, 118],\n",
      "        [ 89, 101, 127]]], dtype=uint8), array([[[244, 246, 251],\n",
      "        [244, 246, 251],\n",
      "        [244, 246, 251],\n",
      "        ...,\n",
      "        [244, 245, 253],\n",
      "        [244, 245, 253],\n",
      "        [244, 245, 253]],\n",
      "\n",
      "       [[244, 246, 251],\n",
      "        [244, 246, 251],\n",
      "        [244, 246, 251],\n",
      "        ...,\n",
      "        [244, 245, 253],\n",
      "        [244, 245, 253],\n",
      "        [244, 245, 253]],\n",
      "\n",
      "       [[244, 246, 251],\n",
      "        [244, 246, 251],\n",
      "        [244, 246, 251],\n",
      "        ...,\n",
      "        [244, 245, 253],\n",
      "        [244, 245, 253],\n",
      "        [244, 245, 253]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[194, 204, 230],\n",
      "        [193, 203, 229],\n",
      "        [191, 201, 226],\n",
      "        ...,\n",
      "        [207, 217, 242],\n",
      "        [198, 208, 232],\n",
      "        [128, 141, 170]],\n",
      "\n",
      "       [[196, 206, 231],\n",
      "        [194, 204, 230],\n",
      "        [192, 202, 227],\n",
      "        ...,\n",
      "        [200, 210, 234],\n",
      "        [132, 145, 175],\n",
      "        [112, 126, 155]],\n",
      "\n",
      "       [[197, 207, 232],\n",
      "        [194, 204, 230],\n",
      "        [192, 202, 227],\n",
      "        ...,\n",
      "        [201, 210, 234],\n",
      "        [144, 157, 184],\n",
      "        [155, 166, 186]]], dtype=uint8)]\n",
      "[[13, 8, 175, 216], [941, 673, 946, 667], [866, 1316, 870, 1286], [388, 101, 105, 305]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_dataset(data_dir):\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(data_dir, filename)) as json_file:\n",
    "                data = json.load(json_file)\n",
    "                # Assuming your images are stored in the same directory with same name but .jpg extension\n",
    "                image_path = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "                if os.path.exists(image_path):\n",
    "                    images.append(cv2.imread(image_path))\n",
    "                    # Extracting coordinates for nose tip and mouth\n",
    "                    nose_x = data[\"nose_tip\"][\"coordinates\"][\"x\"]\n",
    "                    nose_y = data[\"nose_tip\"][\"coordinates\"][\"y\"]\n",
    "                    mouth_x = data[\"mouth\"][\"coordinates\"][\"x\"]\n",
    "                    mouth_y = data[\"mouth\"][\"coordinates\"][\"y\"]\n",
    "                    annotations.append([nose_x, nose_y, mouth_x, mouth_y])\n",
    "\n",
    "    return images, annotations\n",
    "\n",
    "# Load your dataset\n",
    "data_dir = \"/home/selvatharrun/Documents/GitHub/cephalometric-analysis/train3\"\n",
    "images, annotations = load_dataset(data_dir)\n",
    "print(images)\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import json\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "# Define the model\n",
    "def create_custom_model(input_shape):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(4, activation='linear'))  # Output shape is [nose_x, nose_y, mouth_x, mouth_y]\n",
    "    return model\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, y_train = [], []\n",
    "target_size = (64, 64)  # Adjust this based on your model input shape\n",
    "\n",
    "\n",
    "X_train = np.array(images)\n",
    "y_train = np.array(annotations)\n",
    "\n",
    "# Create the model\n",
    "model = create_custom_model(input_shape=X_train.shape[1:])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "input_image = r\"/Users/harsunpranavrs/Downloads/hehe.jpg\"\n",
    "predictions = model.predict(input_image)\n",
    "\n",
    "# Extract nose and mouth coordinates from predictions\n",
    "nose_x, nose_y, mouth_x, mouth_y = predictions.flatten()\n",
    "\n",
    "print(\"Predicted Nose Coordinates:\", nose_x, nose_y)\n",
    "print(\"Predicted Mouth Coordinates:\", mouth_x, mouth_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
